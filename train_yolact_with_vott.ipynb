{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train YOLACT on VoTT annotated images\n",
    "\n",
    "This notebook was tested in docker image `pytorch/pytorch:0.4.1-cuda9-cudnn7-runtime` with 4G shared memory.\n",
    "\n",
    "0. [Install](#0.-Install)\n",
    "  1. [Install PyTorch](#0.1-Install-PyTorch)\n",
    "  2. [Install some other packages](#0.2-Install-some-other-packages)\n",
    "1. [Prepare pretrained weights and dataset](#1.-Prepare-dataset-and-weights)\n",
    "  1. [Download pretrained weights](#1.1-Download-weights)\n",
    "  2. [Prepare dataset](#1.2-Prepare-dataset)\n",
    "  3. [Modify the config file](#1.3-Write-config-file)\n",
    "2. [Train](#2.-Train)\n",
    "3. [Evaluate](#3.-Evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Install PyTorch\n",
    "\n",
    "Install [PyTorch](http://pytorch.org/) 1.0.1 (or higher) and TorchVision.\n",
    "\n",
    "### 0.2 Install some other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install opencv dependencies\n",
    "!apt-get update\n",
    "!apt-get -y install libglib2.0-0 libsm6 libxrender1 libxext-dev\n",
    "# Install yolact dependencies\n",
    "!pip install cython\n",
    "!pip install opencv-python pillow pycocotools matplotlib\n",
    "# Download yolact\n",
    "!git clone https://github.com/dbolya/yolact yolact\n",
    "!mkdir -p yolact/weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare pretrained weights and dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Download pretrained weights\n",
    "Download the imagenet-pretrained model from [here](https://drive.google.com/file/d/1tvqFPd4bJtakOlmn-uIA492g2qurRChj/view?usp=sharing) and put it in `yolact/weights`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Prepare dataset\n",
    "\n",
    "Annotate images in [VoTT](https://github.com/microsoft/VoTT), export the project, and set the path as shown below. If you do not have annotated images, there are some [Azure logos](https://github.com/microsoft/AIVisualProvision/tree/master/Documents/Images/Training_DataSet) with VoTT annotations for you to try out. 4 out of 15 logos are tagged with polygon for instance segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you want to use Azure logo images\n",
    "!git clone https://github.com/microsoft/AIVisualProvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform VoTT annotation into YOLACT format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "########################################\n",
    "# Set paths\n",
    "########################################\n",
    "\n",
    "base_dir = os.path.join(os.getcwd())\n",
    "# The path of VoTT project exported in VoTT JSON format\n",
    "vott_path = os.path.join(base_dir, \"vott\", \"vott-json-export\", \"logo_seg-export.json\")\n",
    "# Raw data path\n",
    "data_path = os.path.join(base_dir, \"AIVisualProvision\", \"Documents\", \"Images\", \"Training_DataSet\")\n",
    "# Processed data path\n",
    "train_image_path = data_path\n",
    "valid_image_path = data_path\n",
    "# The output YOLACT info path\n",
    "train_info_path = os.path.join(base_dir, \"yolact\", \"yolact_info.json\")\n",
    "valid_info_path = train_info_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "from PIL import Image, ExifTags\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "with open(vott_path, \"r\") as f:\n",
    "    vott = json.load(f)\n",
    "\n",
    "########################################\n",
    "# Prepare images\n",
    "########################################\n",
    "\n",
    "print(\"Processing images...\", end=\"\")\n",
    "\n",
    "def rotate(origin, point, angle):\n",
    "    \"\"\"\n",
    "    Rotate a point counterclockwise by a given angle (0/90/180/270) around a given origin.\n",
    "    \"\"\"\n",
    "    ox, oy = origin\n",
    "    px, py = point\n",
    "\n",
    "    if angle == 90:\n",
    "        qx, qy = oy - (py - oy), ox + (px - ox)\n",
    "    elif angle == 180:\n",
    "        qx, qy = ox - (px - ox), oy - (py - oy)\n",
    "    elif angle == 270:\n",
    "        qx, qy = oy + (py - oy), ox - (px - ox)\n",
    "    elif angle in [0, 360]:\n",
    "        qx, qy = px, py\n",
    "    #qx = ox + math.cos(angle) * (px) - math.sin(angle) * (py)\n",
    "    #qy = oy + math.sin(angle) * (px) + math.cos(angle) * (py)\n",
    "    return qx, qy\n",
    "\n",
    "vott_images = [image for image in vott[\"assets\"].values() if image[\"asset\"][\"format\"] in [\"jpeg\", \"jpg\", \"png\"]]\n",
    "\n",
    "class_names = tuple(tag[\"name\"] for tag in vott[\"tags\"])\n",
    "label_map = {tag[\"name\"]:i for i, tag in enumerate(vott[\"tags\"], 1)}\n",
    "#yolcal_label_map = {i:label_map[] for i, class_id in enumerate(class_names)}\n",
    "\n",
    "orientation_key = [k for k, v in ExifTags.TAGS.items() if v == 'Orientation'][0]\n",
    "orientation_map = {3:180, 6:90, 8:270}\n",
    "images = list()\n",
    "for i_im, image in enumerate(vott_images, 1):\n",
    "    #print(\"\\rProcessing images {}/{}\".format(i_im, len(vott_images)), end=\"\")\n",
    "    \n",
    "    file_name = image[\"asset\"][\"name\"].replace(\"%20\", \" \")\n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    img = Image.open(file_path)\n",
    "    orientation = None if img._getexif() is None else img._getexif().get(orientation_key, None)\n",
    "    yolact_image = {\n",
    "        \"id\" : int(image[\"asset\"][\"id\"], 16),\n",
    "        \"orientation\" : orientation,\n",
    "        \"file_name\" : file_name,\n",
    "        \"license\" : None,\n",
    "        \"flickr_url\" : None,\n",
    "        \"coco_url\" : None,\n",
    "        \"date_captured\" : None,\n",
    "    }\n",
    "    if orientation not in [6, 8]:\n",
    "        yolact_image.update(image[\"asset\"][\"size\"])\n",
    "    else:\n",
    "        yolact_image.update({\n",
    "            \"width\" : image[\"asset\"][\"size\"][\"height\"],\n",
    "            \"height\" : image[\"asset\"][\"size\"][\"width\"],\n",
    "        })\n",
    "    images.append(yolact_image)\n",
    "\n",
    "print(\"\\rProcessed \"+str(len(vott_images))+\" images.\")\n",
    "\n",
    "########################################\n",
    "# Process annotations\n",
    "########################################\n",
    "\n",
    "print(\"Processing annotations...\", end=\"\")\n",
    "\n",
    "def rotate_annotations(vott_points, angle, size):\n",
    "    \n",
    "    origin = [size[\"width\"]//2, size[\"height\"]//2]\n",
    "    return [\n",
    "        {k:v for k, v in zip(\n",
    "            [\"x\", \"y\"],\n",
    "            rotate(origin, [p[\"x\"], p[\"y\"]], angle))} \\\n",
    "        for p in vott_points\n",
    "    ]\n",
    "\n",
    "def get_poly_area(x,y):\n",
    "    return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))\n",
    "\n",
    "def rotate_bbox(bbox, angle, size):\n",
    "    origin = [size[\"width\"]//2, size[\"height\"]//2]\n",
    "    \n",
    "    points = [\n",
    "        [bbox[\"left\"], bbox[\"left\"], bbox[\"left\"]+bbox[\"width\"], bbox[\"left\"]+bbox[\"width\"]],\n",
    "        [bbox[\"top\"], bbox[\"top\"]+bbox[\"height\"], bbox[\"top\"], bbox[\"top\"]+bbox[\"height\"]]\n",
    "    ]\n",
    "    points = [rotate(origin, p, angle) for p in zip(*points)]\n",
    "    xs, ys = zip(*points)\n",
    "    left, top = min(xs), min(ys)\n",
    "    width, height = max(xs) - left, max(ys) - top\n",
    "    \n",
    "    return left, top, width, height\n",
    "\n",
    "annotations = [{\n",
    "    \"id\" : region[\"id\"],\n",
    "    \"image_id\" : int(image[\"asset\"][\"id\"], 16),\n",
    "    \"category_id\" : label_map[tag],\n",
    "    \"segmentation\" : [[\n",
    "        coor for point in rotate_annotations(\n",
    "            region[\"points\"],\n",
    "            orientation_map.get(yolact_image[\"orientation\"], 0),\n",
    "            image[\"asset\"][\"size\"]) \\\n",
    "        for coor in [point[\"x\"], point[\"y\"]]\n",
    "    ]],\n",
    "    \"area\" : get_poly_area(*zip(*[(point[\"x\"], point[\"y\"]) for point in region[\"points\"]])),\n",
    "    #\"bbox\" : [region[\"boundingBox\"][k] for k in [\"left\", \"top\", \"width\", \"height\"]],\n",
    "    \"bbox\" : rotate_bbox(\n",
    "        region[\"boundingBox\"],\n",
    "        orientation_map.get(yolact_image[\"orientation\"], 0),\n",
    "        image[\"asset\"][\"size\"]),\n",
    "    \"iscrowd\" : 0,\n",
    "} for image, yolact_image in zip(vott_images, images) for region in image[\"regions\"] for tag in region[\"tags\"]]\n",
    "\n",
    "print(\"\\rProcessed \"+str(len(annotations))+\" annotations.\")\n",
    "\n",
    "########################################\n",
    "# Wrap-up\n",
    "########################################\n",
    "\n",
    "yolact_info = {\n",
    "    \"images\" : images,\n",
    "    \"annotations\" : annotations\n",
    "}\n",
    "\n",
    "with open(train_info_path, \"w\") as f:\n",
    "    json.dump(yolact_info, f)\n",
    "with open(valid_info_path, \"w\") as f:\n",
    "    json.dump(yolact_info, f)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Modify the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "config_path = os.path.join('yolact', 'data', 'config.py')\n",
    "config_add_txt = \"\"\"\n",
    "\n",
    "# Custom dataset config\n",
    "my_yolact_dataset = dataset_base.copy({{\n",
    "    'name' : 'vott2yolact',\n",
    "    'train_images' : '{}',\n",
    "    'train_info' : '{}',\n",
    "    'valid_images' : '{}',\n",
    "    'valid_info' : '{}',\n",
    "    'has_gt' : True,\n",
    "    'class_names' : {},\n",
    "}})\n",
    "\n",
    "my_yolact_config = yolact_base_config.copy({{\n",
    "    'name': 'my_yolact',\n",
    "    'dataset': my_yolact_dataset,\n",
    "    'num_classes': len(my_yolact_dataset.class_names) + 1,\n",
    "    'max_iter': 2000,\n",
    "    'max_size': 400,\n",
    "}})\n",
    "\"\"\".format(train_image_path, train_info_path, valid_image_path, valid_info_path, class_names[:4])\n",
    "\n",
    "with open(config_path, 'a') as f:\n",
    "    f.write(config_add_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train\n",
    "\n",
    "Fill your custom config name in `--config` and retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cd yolact\n",
    "!CUDA_VISIBLE_DEVICES=0 python train.py \\\n",
    "    --config=my_yolact_config \\\n",
    "    --start_iter=-1 \\\n",
    "    --validation_epoch=10 \\\n",
    "    --keep_latest\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate\n",
    "\n",
    "Find the trained weights in `yolact/weights` and fill the file name in `--trained_model`. The weight file should have the custom config name as its prefix. Then provide the evaluate image and its output path in `--image`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd yolact\n",
    "!CUDA_VISIBLE_DEVICES=0 python eval.py \\\n",
    "    --trained_model=weights/my_yolact_199_2000.pth \\\n",
    "    --score_threshold=0.5 \\\n",
    "    --top_k=100 \\\n",
    "    --image=\"../AIVisualProvision/Documents/Images/Training_DataSet/allmagnets.jpg:../output_image.jpg\"\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "px = []\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(Image.open(\"output_image.jpg\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
